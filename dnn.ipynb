{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN classifier\n",
    "\n",
    "reference data: https://labs.criteo.com/2013/12/download-terabyte-click-logs-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from os import path\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import PreprocessingLayer, TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBL_COLUMN = ['lbl']\n",
    "CAT_COLUMNS = [f'cat{i}' for i in range(26)]\n",
    "NUM_COLUMNS = [f'num{i}' for i in range(13)]\n",
    "COLUMNS = LBL_COLUMN + NUM_COLUMNS + CAT_COLUMNS\n",
    "FEATURE_COLUMNS = NUM_COLUMNS + CAT_COLUMNS\n",
    "COLUMN_DEFAULTS = [0]*14 + ['thisisdefault']*26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern='data/dac/sample_train.txt', \n",
    "    batch_size=200,\n",
    "    num_epochs=1,\n",
    "    column_defaults=COLUMN_DEFAULTS,\n",
    "    column_names=COLUMNS, \n",
    "    label_name='lbl', \n",
    "    field_delim='\\t',\n",
    "    shuffle=True\n",
    ")\\\n",
    ".shuffle(10, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Conbine the columes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesExtraction:\n",
    "    \n",
    "    def __init__(self, num_col, feature_type=None):\n",
    "        self.num_col = num_col\n",
    "        self.feature_type = feature_type\n",
    "\n",
    "    def __call__(self, features, labels):\n",
    "        numeric_features = [features.pop(col) for col in self.num_col]\n",
    "        numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "        numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "        \n",
    "        if self.feature_type == 'cat':\n",
    "            return features\n",
    "        if self.feature_type == 'numeric':\n",
    "            return numeric_features\n",
    "        if self.feature_type == 'no_label':\n",
    "            features['numeric'] = numeric_features\n",
    "            return features\n",
    "        else:\n",
    "            features['numeric'] = numeric_features\n",
    "            return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_dataset = dataset.map(FeaturesExtraction(num_col=NUM_COLUMNS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Training / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_validate(idx, data):\n",
    "    return idx % 5 == 0\n",
    "\n",
    "def is_train(idx, data):\n",
    "    return not is_validate(idx, data)\n",
    "\n",
    "recover = lambda idx, data: data\n",
    "\n",
    "validate_dataset = packed_dataset.enumerate()\\\n",
    ".filter(is_validate)\\\n",
    ".map(recover)\n",
    "\n",
    "train_dataset = packed_dataset.enumerate()\\\n",
    ".filter(is_train)\\\n",
    ".map(recover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessingLayer(PreprocessingLayer):\n",
    "    \n",
    "    def __init__(self, ls_cat_col, num_col, vocabulary=None, **kwargs):\n",
    "        \n",
    "        super(DataProcessingLayer, self).__init__(**kwargs)\n",
    "        self._ls_cat_col = ls_cat_col\n",
    "        self._num_col = num_col\n",
    "        self._dict_cat_vocab = vocabulary\n",
    "        \n",
    "        self._dict_vectorization_layer = dict()\n",
    "        for key in ls_cat_col:\n",
    "            self._dict_vectorization_layer.update({\n",
    "                key: TextVectorization(output_sequence_length=1)\n",
    "            })\n",
    "        self.processing_layer = None\n",
    "    \n",
    "    def adapt(self, data):\n",
    "        \n",
    "        for cat_col in self._ls_cat_col:\n",
    "            print(f'adapting col: {cat_col}')\n",
    "            tmp_dataset = data.map(lambda feature, label: feature.pop(cat_col))\n",
    "            self._dict_vectorization_layer[cat_col].adapt(tmp_dataset)\n",
    "        \n",
    "        self._dict_cat_vocab = dict()\n",
    "        for cat_col in self._ls_cat_col:\n",
    "            self._dict_cat_vocab.update({\n",
    "                cat_col: self._dict_vectorization_layer[cat_col].get_vocabulary()\n",
    "            })\n",
    "                \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        ls_feature_cat_col = list()\n",
    "        ls_feature_num_col = list()\n",
    "        \n",
    "        for cat_col, vocab in self._dict_cat_vocab.items():\n",
    "            tf_cat_vocab = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "                key=cat_col, vocabulary_list=vocab)\n",
    "            ls_feature_cat_col.append(tf.feature_column.indicator_column(tf_cat_vocab))\n",
    "        \n",
    "        tf_feature_num = tf.feature_column.numeric_column(self._num_col, shape=input_shape[self._num_col].as_list()[-1])\n",
    "        ls_feature_num_col.append(tf_feature_num)\n",
    "        \n",
    "        self.processing_layer = tf.keras.layers.DenseFeatures(ls_feature_cat_col + ls_feature_num_col)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        return self.processing_layer(inputs)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super(DataProcessingLayer, self).get_config()\n",
    "        config.update({\n",
    "            'ls_cat_col': self._ls_cat_col,\n",
    "            'num_col': self._num_col,\n",
    "            'vocabulary': self._dict_cat_vocab\n",
    "        })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \n",
    "        return cls(**config)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_processing_layer(file_path, dataset, re_build=True):\n",
    "\n",
    "    if path.exists(file_path) and re_build == False:\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            serialized_layer = pickle.load(f) \n",
    "        layer = tf.keras.layers.deserialize(\n",
    "            serialized_layer, custom_objects={'DataProcessingLayer': DataProcessingLayer}\n",
    "        )\n",
    "        \n",
    "        print(f'Loaded the saved layer from {file_path}')\n",
    "\n",
    "        return layer\n",
    "\n",
    "    else:\n",
    "\n",
    "        layer = DataProcessingLayer(\n",
    "            ls_cat_col=CAT_COLUMNS,\n",
    "            num_col='numeric',\n",
    "            name='data_processing_layer'\n",
    "        )\n",
    "        layer.adapt(data=dataset)\n",
    "        serialized_layer = tf.keras.layers.serialize(layer)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(serialized_layer, f)\n",
    "            \n",
    "        print(f'Saved the layer to {file_path}')\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ANN Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_hiden_layer, ls_hiden_unit, re_build_onhot=False, **kwargs):\n",
    "        \n",
    "        super(ANNModel, self).__init__(**kwargs)\n",
    "        \n",
    "        assert num_hiden_layer == len(ls_hiden_unit), f'num_hiden_layer != len(ls_hiden_unit)'\n",
    "        self.num_hiden_layer = num_hiden_layer\n",
    "        \n",
    "        # processing layer\n",
    "        self.data_processing_layer = build_data_processing_layer(\n",
    "            file_path='saved/layer/DataProcessingLayer.pkl',\n",
    "            dataset=train_dataset,\n",
    "            re_build=re_build_onhot\n",
    "        )\n",
    "        \n",
    "        # fully connected hiden layers\n",
    "        self.ls_hiden_layer = list()\n",
    "        for i in range(num_hiden_layer):\n",
    "            self.ls_hiden_layer.append(\n",
    "                tf.keras.layers.Dense(\n",
    "                    ls_hiden_unit[i], \n",
    "                    activation='relu', \n",
    "                    name=f'hiden_layer_{i}'))\n",
    "            \n",
    "        # output layer\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='output_layer')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.data_processing_layer(inputs)\n",
    "        for i in range(self.num_hiden_layer):\n",
    "            x = self.ls_hiden_layer[i](x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        \n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True) \n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        \n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        \n",
    "        # Compute predictions\n",
    "        y_pred = self(x, training=False)\n",
    "        \n",
    "        # Updates the metrics tracking the loss\n",
    "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        \n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the saved layer from saved/layer/DataProcessingLayer.pkl\n"
     ]
    }
   ],
   "source": [
    "ann_model = ANNModel(\n",
    "    num_hiden_layer=2,\n",
    "    ls_hiden_unit=[128,128],\n",
    "    name='binary_classifer'\n",
    ")\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "ann_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf saved/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'saved/model/checkpoint'\n",
    "checkpoint_dir = path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0)\n",
    "\n",
    "# Create a callback for TensorBoard\n",
    "logdir = \"saved/logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.6707712322473526)\n"
     ]
    }
   ],
   "source": [
    "model_history = ann_model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=validate_dataset, \n",
    "    callbacks=[cp_callback, tensorboard_callback],\n",
    "    epochs=20,\n",
    "    verbose=0,\n",
    "    workers=4)\n",
    "\n",
    "print(f'Average test loss: {np.average(model_history.history[\"loss\"])})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"binary_classifer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data_processing_layer (DataP multiple                  0         \n",
      "_________________________________________________________________\n",
      "hiden_layer_0 (Dense)        multiple                  271872    \n",
      "_________________________________________________________________\n",
      "hiden_layer_1 (Dense)        multiple                  16512     \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         multiple                  129       \n",
      "=================================================================\n",
      "Total params: 288,513\n",
      "Trainable params: 288,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Load testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern='data/dac/sample_test.txt', \n",
    "    batch_size=200,\n",
    "    num_epochs=1,\n",
    "    column_defaults=COLUMN_DEFAULTS,\n",
    "    column_names=COLUMNS, \n",
    "    label_name='lbl', \n",
    "    field_delim='\\t',\n",
    "    shuffle=True\n",
    ").shuffle(10, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_packed_dataset = testing_dataset.map(FeaturesExtraction(num_col=NUM_COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.6679402589797974,\n",
       " 'tp': 793.0,\n",
       " 'fp': 0.0,\n",
       " 'tn': 9513.0,\n",
       " 'fn': 1645.0,\n",
       " 'accuracy': 0.8623546361923218,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.32526659965515137,\n",
       " 'auc': 0.6626332998275757}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model.evaluate(testing_packed_dataset, return_dict=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binary_classifier_lab",
   "language": "python",
   "name": "binary_classifier_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
