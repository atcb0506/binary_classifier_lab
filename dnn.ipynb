{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN classifier\n",
    "\n",
    "reference data: https://labs.criteo.com/2013/12/download-terabyte-click-logs-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from os import path\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import PreprocessingLayer, TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBL_COLUMN = ['lbl']\n",
    "CAT_COLUMNS = [f'cat{i}' for i in range(26)]\n",
    "NUM_COLUMNS = [f'num{i}' for i in range(13)]\n",
    "COLUMNS = LBL_COLUMN + NUM_COLUMNS + CAT_COLUMNS\n",
    "FEATURE_COLUMNS = NUM_COLUMNS + CAT_COLUMNS\n",
    "COLUMN_DEFAULTS = [0]*14 + ['thisisdefault']*26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern='data/dac/sample_train.txt', \n",
    "    batch_size=200,\n",
    "    num_epochs=1,\n",
    "    column_defaults=COLUMN_DEFAULTS,\n",
    "    column_names=COLUMNS, \n",
    "    label_name='lbl', \n",
    "    field_delim='\\t',\n",
    "    shuffle=True\n",
    ")\\\n",
    ".shuffle(10, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Conbine the columes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesExtraction:\n",
    "    \n",
    "    def __init__(self, num_col, feature_type=None):\n",
    "        self.num_col = num_col\n",
    "        self.feature_type = feature_type\n",
    "\n",
    "    def __call__(self, features, labels):\n",
    "        numeric_features = [features.pop(col) for col in self.num_col]\n",
    "        numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "        numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "        \n",
    "        if self.feature_type == 'cat':\n",
    "            return features\n",
    "        if self.feature_type == 'numeric':\n",
    "            return numeric_features\n",
    "        if self.feature_type == 'no_label':\n",
    "            features['numeric'] = numeric_features\n",
    "            return features\n",
    "        else:\n",
    "            features['numeric'] = numeric_features\n",
    "            return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_dataset = dataset.map(FeaturesExtraction(num_col=NUM_COLUMNS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Training / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_validate(idx, data):\n",
    "    return idx % 5 == 0\n",
    "\n",
    "def is_train(idx, data):\n",
    "    return not is_validate(idx, data)\n",
    "\n",
    "recover = lambda idx, data: data\n",
    "\n",
    "validate_dataset = packed_dataset.enumerate()\\\n",
    ".filter(is_validate)\\\n",
    ".map(recover)\n",
    "\n",
    "train_dataset = packed_dataset.enumerate()\\\n",
    ".filter(is_train)\\\n",
    ".map(recover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessingLayer(PreprocessingLayer):\n",
    "    \n",
    "    def __init__(self, ls_cat_col, num_col, vocabulary=None, **kwargs):\n",
    "        \n",
    "        super(DataProcessingLayer, self).__init__(**kwargs)\n",
    "        self._ls_cat_col = ls_cat_col\n",
    "        self._num_col = num_col\n",
    "        self._dict_cat_vocab = vocabulary\n",
    "        \n",
    "        self._dict_vectorization_layer = dict()\n",
    "        for key in ls_cat_col:\n",
    "            self._dict_vectorization_layer.update({\n",
    "                key: TextVectorization(output_sequence_length=1)\n",
    "            })\n",
    "        self.processing_layer = None\n",
    "    \n",
    "    def adapt(self, data):\n",
    "        \n",
    "        for cat_col in self._ls_cat_col:\n",
    "            print(f'adapting col: {cat_col}')\n",
    "            tmp_dataset = data.map(lambda feature, label: feature.pop(cat_col))\n",
    "            self._dict_vectorization_layer[cat_col].adapt(tmp_dataset)\n",
    "        \n",
    "        self._dict_cat_vocab = dict()\n",
    "        for cat_col in self._ls_cat_col:\n",
    "            self._dict_cat_vocab.update({\n",
    "                cat_col: self._dict_vectorization_layer[cat_col].get_vocabulary()\n",
    "            })\n",
    "                \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        ls_feature_cat_col = list()\n",
    "        ls_feature_num_col = list()\n",
    "        \n",
    "        for cat_col, vocab in self._dict_cat_vocab.items():\n",
    "            tf_cat_vocab = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "                key=cat_col, vocabulary_list=vocab)\n",
    "            ls_feature_cat_col.append(tf.feature_column.indicator_column(tf_cat_vocab))\n",
    "        \n",
    "        tf_feature_num = tf.feature_column.numeric_column(self._num_col, shape=input_shape[self._num_col].as_list()[-1])\n",
    "        ls_feature_num_col.append(tf_feature_num)\n",
    "        \n",
    "        self.processing_layer = tf.keras.layers.DenseFeatures(ls_feature_cat_col + ls_feature_num_col)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        return self.processing_layer(inputs)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super(DataProcessingLayer, self).get_config()\n",
    "        config.update({\n",
    "            'ls_cat_col': self._ls_cat_col,\n",
    "            'num_col': self._num_col,\n",
    "            'vocabulary': self._dict_cat_vocab\n",
    "        })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \n",
    "        return cls(**config)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_processing_layer(file_path, dataset, re_build=True):\n",
    "\n",
    "    if path.exists(file_path) and re_build == False:\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            serialized_layer = pickle.load(f) \n",
    "        layer = tf.keras.layers.deserialize(\n",
    "            serialized_layer, custom_objects={'DataProcessingLayer': DataProcessingLayer}\n",
    "        )\n",
    "        \n",
    "        print(f'Loaded the saved layer from {file_path}')\n",
    "\n",
    "        return layer\n",
    "\n",
    "    else:\n",
    "\n",
    "        layer = DataProcessingLayer(\n",
    "            ls_cat_col=CAT_COLUMNS,\n",
    "            num_col='numeric',\n",
    "            name='data_processing_layer'\n",
    "        )\n",
    "        layer.adapt(data=dataset)\n",
    "        serialized_layer = tf.keras.layers.serialize(layer)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(serialized_layer, f)\n",
    "            \n",
    "        print(f'Saved the layer to {file_path}')\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the saved layer from saved/layer/DataProcessingLayer.pkl\n"
     ]
    }
   ],
   "source": [
    "data_processing_layer = build_data_processing_layer(\n",
    "    file_path='saved/layer/DataProcessingLayer.pkl',\n",
    "    dataset=train_dataset,\n",
    "    re_build=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ANN Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model_input = {\n",
    "    'numeric': tf.keras.Input((len(NUM_COLUMNS),), dtype=tf.dtypes.float32, name='numeric_input')\n",
    "}\n",
    "\n",
    "for cat_column in CAT_COLUMNS:\n",
    "    ann_model_input.update({cat_column: tf.keras.Input((1,), dtype=tf.dtypes.string, name=f'{cat_column}_input')})\n",
    "\n",
    "x = data_processing_layer(ann_model_input)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name='hiden_layer_1')(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name='hiden_layer_2')(x)\n",
    "ann_model_output = tf.keras.layers.Dense(1, activation='sigmoid', name='output_layer')(x)\n",
    "\n",
    "ann_model = tf.keras.Model(\n",
    "    inputs=ann_model_input, \n",
    "    outputs=ann_model_output, \n",
    "    name='binary_classifer')\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "ann_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"binary_classifer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cat0_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat1_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat10_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat11_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat12_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat13_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat14_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat15_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat16_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat17_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat18_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat19_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat2_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat20_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat21_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat22_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat23_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat24_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat25_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat3_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat4_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat5_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat6_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat7_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat8_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat9_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "numeric_input (InputLayer)      [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "data_processing_layer (DataProc (None, 2123)         0           cat0_input[0][0]                 \n",
      "                                                                 cat1_input[0][0]                 \n",
      "                                                                 cat10_input[0][0]                \n",
      "                                                                 cat11_input[0][0]                \n",
      "                                                                 cat12_input[0][0]                \n",
      "                                                                 cat13_input[0][0]                \n",
      "                                                                 cat14_input[0][0]                \n",
      "                                                                 cat15_input[0][0]                \n",
      "                                                                 cat16_input[0][0]                \n",
      "                                                                 cat17_input[0][0]                \n",
      "                                                                 cat18_input[0][0]                \n",
      "                                                                 cat19_input[0][0]                \n",
      "                                                                 cat2_input[0][0]                 \n",
      "                                                                 cat20_input[0][0]                \n",
      "                                                                 cat21_input[0][0]                \n",
      "                                                                 cat22_input[0][0]                \n",
      "                                                                 cat23_input[0][0]                \n",
      "                                                                 cat24_input[0][0]                \n",
      "                                                                 cat25_input[0][0]                \n",
      "                                                                 cat3_input[0][0]                 \n",
      "                                                                 cat4_input[0][0]                 \n",
      "                                                                 cat5_input[0][0]                 \n",
      "                                                                 cat6_input[0][0]                 \n",
      "                                                                 cat7_input[0][0]                 \n",
      "                                                                 cat8_input[0][0]                 \n",
      "                                                                 cat9_input[0][0]                 \n",
      "                                                                 numeric_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hiden_layer_1 (Dense)           (None, 128)          271872      data_processing_layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hiden_layer_2 (Dense)           (None, 128)          16512       hiden_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1)            129         hiden_layer_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 288,513\n",
      "Trainable params: 288,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf saved/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"saved/model/cp.ckpt\"\n",
    "checkpoint_dir = path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Create a callback for TensorBoard\n",
    "logdir = \"saved/logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... ('Not JSON Serializable:', b'05db9164')\n",
      "\n",
      "Epoch 00001: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00002: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00003: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00004: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00005: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00006: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00007: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00008: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00009: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00010: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00011: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00012: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00013: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00014: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00015: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00016: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00017: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00018: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00019: saving model to saved/model/cp.ckpt\n",
      "\n",
      "Epoch 00020: saving model to saved/model/cp.ckpt\n",
      "Average test loss: 0.6780615389347077)\n"
     ]
    }
   ],
   "source": [
    "model_history = ann_model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=validate_dataset, \n",
    "    callbacks=[cp_callback, tensorboard_callback],\n",
    "    epochs=20,\n",
    "    verbose=0,\n",
    "    workers=4)\n",
    "\n",
    "print(f'Average test loss: {np.average(model_history.history[\"loss\"])})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Load testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern='data/dac/sample_test.txt', \n",
    "    batch_size=200,\n",
    "    num_epochs=1,\n",
    "    column_defaults=COLUMN_DEFAULTS,\n",
    "    column_names=COLUMNS, \n",
    "    label_name='lbl', \n",
    "    field_delim='\\t',\n",
    "    shuffle=True\n",
    ").shuffle(10, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_packed_dataset = testing_dataset.map(FeaturesExtraction(num_col=NUM_COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.6737586855888367,\n",
       " 'tp': 610.0,\n",
       " 'fp': 0.0,\n",
       " 'tn': 9513.0,\n",
       " 'fn': 1828.0,\n",
       " 'accuracy': 0.8470420837402344,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.2502050995826721,\n",
       " 'auc': 0.6251025199890137}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model.evaluate(testing_packed_dataset, return_dict=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binary_classifier_lab",
   "language": "python",
   "name": "binary_classifier_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
