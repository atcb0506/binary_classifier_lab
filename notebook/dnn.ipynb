{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN classifier\n",
    "\n",
    "reference data: https://labs.criteo.com/2013/12/download-terabyte-click-logs-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from os import path\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import functools\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import PreprocessingLayer, TextVectorization, Normalization\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBL_COLUMN = ['lbl']\n",
    "CAT_COLUMNS = [f'cat{i}' for i in range(26)]\n",
    "NUM_COLUMNS = [f'num{i}' for i in range(13)]\n",
    "COLUMNS = LBL_COLUMN + NUM_COLUMNS + CAT_COLUMNS\n",
    "FEATURE_COLUMNS = NUM_COLUMNS + CAT_COLUMNS\n",
    "COLUMN_DEFAULTS = [0]*14 + ['thisisdefault']*26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern='../data/dac/sample_train.txt', \n",
    "    batch_size=200,\n",
    "    num_epochs=1,\n",
    "    column_defaults=COLUMN_DEFAULTS,\n",
    "    column_names=COLUMNS, \n",
    "    label_name='lbl', \n",
    "    field_delim='\\t',\n",
    "    shuffle=True\n",
    ")\\\n",
    ".shuffle(10, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Conbine the columes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesExtraction:\n",
    "    \n",
    "    def __init__(self, num_col, feature_type=None):\n",
    "        self.num_col = num_col\n",
    "        self.feature_type = feature_type\n",
    "\n",
    "    def __call__(self, features, labels):\n",
    "        numeric_features = [features.pop(col) for col in self.num_col]\n",
    "        numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "        numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "        \n",
    "        if self.feature_type == 'cat':\n",
    "            return features\n",
    "        if self.feature_type == 'numeric':\n",
    "            return numeric_features\n",
    "        if self.feature_type == 'no_label':\n",
    "            features['numeric'] = numeric_features\n",
    "            return features\n",
    "        else:\n",
    "            features['numeric'] = numeric_features\n",
    "            return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_dataset = dataset.map(FeaturesExtraction(num_col=NUM_COLUMNS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Training / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_validate(idx, _):\n",
    "    return idx % 5 == 0\n",
    "\n",
    "def is_train(idx, data):\n",
    "    return not is_validate(idx, data)\n",
    "\n",
    "def recover(_, data):\n",
    "    return data\n",
    "\n",
    "validate_dataset = packed_dataset.enumerate()\\\n",
    ".filter(is_validate)\\\n",
    ".map(recover)\n",
    "\n",
    "train_dataset = packed_dataset.enumerate()\\\n",
    ".filter(is_train)\\\n",
    ".map(recover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessingLayer(PreprocessingLayer):\n",
    "    \n",
    "    def __init__(self, ls_cat_col, num_col, mean=None, sd=None, vocabulary=None, **kwargs):\n",
    "        \n",
    "        super(DataProcessingLayer, self).__init__(**kwargs)\n",
    "        self._ls_cat_col = ls_cat_col\n",
    "        self._num_col = num_col\n",
    "        self._mean = mean\n",
    "        self._sd = sd\n",
    "        self._dict_cat_vocab = vocabulary\n",
    "        \n",
    "        self._normalization_layer = Normalization()\n",
    "        self._dict_vectorization_layer = dict()\n",
    "        for key in ls_cat_col:\n",
    "            self._dict_vectorization_layer.update({\n",
    "                key: TextVectorization(output_sequence_length=1)\n",
    "            })\n",
    "        self.processing_layer = None\n",
    "    \n",
    "    def adapt(self, data):\n",
    "        \n",
    "        # numeric - calcate mean and sd\n",
    "        print(f'adapting col: {self._num_col}')\n",
    "        tmp_dataset = data.map(lambda feature, label: feature.pop(self._num_col))\n",
    "        self._normalization_layer.adapt(tmp_dataset)\n",
    "        weight = self._normalization_layer.get_weights()\n",
    "        self._mean = weight[0]\n",
    "        self._sd = weight[1]\n",
    "        \n",
    "        # cat - distinct the cat\n",
    "        for cat_col in self._ls_cat_col:\n",
    "            print(f'adapting col: {cat_col}')\n",
    "            tmp_dataset = data.map(lambda feature, label: feature.pop(cat_col))\n",
    "            self._dict_vectorization_layer[cat_col].adapt(tmp_dataset)\n",
    "        self._dict_cat_vocab = dict()\n",
    "        for cat_col in self._ls_cat_col:\n",
    "            self._dict_cat_vocab.update({\n",
    "                cat_col: self._dict_vectorization_layer[cat_col].get_vocabulary()\n",
    "            })\n",
    "                \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        ls_feature_num_col = list()\n",
    "        ls_feature_cat_col = list()\n",
    "        \n",
    "        normalization_udf = functools.partial(\n",
    "            self._normalize_numeric_data, \n",
    "            mean=self._mean, \n",
    "            std=self._sd\n",
    "        )\n",
    "        tf_feature_num = tf.feature_column.numeric_column(\n",
    "            self._num_col, \n",
    "            normalizer_fn=normalization_udf,\n",
    "            shape=input_shape[self._num_col].as_list()[-1]\n",
    "        )\n",
    "        ls_feature_num_col.append(tf_feature_num)\n",
    "        \n",
    "        for cat_col, vocab in self._dict_cat_vocab.items():\n",
    "            tf_cat_vocab = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "                key=cat_col, vocabulary_list=vocab\n",
    "            )\n",
    "            ls_feature_cat_col.append(tf.feature_column.indicator_column(tf_cat_vocab))\n",
    "\n",
    "        \n",
    "        self.processing_layer = tf.keras.layers.DenseFeatures(ls_feature_num_col + ls_feature_cat_col)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        return self.processing_layer(inputs)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super(DataProcessingLayer, self).get_config()\n",
    "        config.update({\n",
    "            'ls_cat_col': self._ls_cat_col,\n",
    "            'num_col': self._num_col,\n",
    "            'mean': self._mean,\n",
    "            'sd': self._sd,\n",
    "            'vocabulary': self._dict_cat_vocab\n",
    "        })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \n",
    "        return cls(**config)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _normalize_numeric_data(data, mean, std):\n",
    "        \n",
    "        return (data-mean)/std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_processing_layer(file_path, dataset, rebuild_proc=False):\n",
    "\n",
    "    if path.exists(file_path) and rebuild_proc == False:\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            serialized_layer = pickle.load(f) \n",
    "        layer = tf.keras.layers.deserialize(\n",
    "            serialized_layer, custom_objects={'DataProcessingLayer': DataProcessingLayer}\n",
    "        )\n",
    "        \n",
    "        print(f'Loaded the saved layer from {file_path}')\n",
    "\n",
    "        return layer\n",
    "\n",
    "    else:\n",
    "\n",
    "        layer = DataProcessingLayer(\n",
    "            ls_cat_col=CAT_COLUMNS,\n",
    "            num_col='numeric',\n",
    "            name='data_processing_layer'\n",
    "        )\n",
    "        layer.adapt(data=dataset)\n",
    "        serialized_layer = tf.keras.layers.serialize(layer)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(serialized_layer, f)\n",
    "            \n",
    "        print(f'Saved the layer to {file_path}')\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ANN Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_hiden_layer, ls_hiden_unit, rebuild_proc=False, **kwargs):\n",
    "        \n",
    "        super(ANNModel, self).__init__(**kwargs)\n",
    "        \n",
    "        assert num_hiden_layer == len(ls_hiden_unit), f'num_hiden_layer != len(ls_hiden_unit)'\n",
    "        self.num_hiden_layer = num_hiden_layer\n",
    "        \n",
    "        # processing layer\n",
    "        self.data_processing_layer = build_data_processing_layer(\n",
    "            file_path='../saved/layer/DataProcessingLayer.pkl',\n",
    "            dataset=train_dataset,\n",
    "            rebuild_proc=rebuild_proc\n",
    "        )\n",
    "        \n",
    "        # fully connected hiden layers\n",
    "        self.ls_hiden_layer = list()\n",
    "        for i in range(num_hiden_layer):\n",
    "            self.ls_hiden_layer.append(\n",
    "                tf.keras.layers.Dense(\n",
    "                    ls_hiden_unit[i], \n",
    "                    activation='relu', \n",
    "                    name=f'hiden_layer_{i}'))\n",
    "            \n",
    "        # output layer\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='output_layer')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.data_processing_layer(inputs)\n",
    "        for i in range(self.num_hiden_layer):\n",
    "            x = self.ls_hiden_layer[i](x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        \n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True) \n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        \n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        \n",
    "        # Compute predictions\n",
    "        y_pred = self(x, training=False)\n",
    "        \n",
    "        # Updates the metrics tracking the loss\n",
    "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        \n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the saved layer from ../saved/layer/DataProcessingLayer.pkl\n"
     ]
    }
   ],
   "source": [
    "ann_model = ANNModel(\n",
    "    num_hiden_layer=2,\n",
    "    ls_hiden_unit=[128,128],\n",
    "    name='binary_classifer'\n",
    ")\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "ann_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf ../saved/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../saved/model/checkpoint'\n",
    "checkpoint_dir = path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0)\n",
    "\n",
    "# Create a callback for TensorBoard\n",
    "logdir = \"../saved/logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "48/48 [==============================] - 4s 77ms/step - loss: 0.6928 - tp: 672.0000 - fp: 186.0000 - tn: 7411.0000 - fn: 1282.0000 - accuracy: 0.8463 - precision: 0.7832 - recall: 0.3439 - auc: 0.9011 - val_loss: 0.6325 - val_tp: 425.0000 - val_fp: 0.0000e+00 - val_tn: 1927.0000 - val_fn: 48.0000 - val_accuracy: 0.9800 - val_precision: 1.0000 - val_recall: 0.8985 - val_auc: 0.9964\n",
      "Epoch 2/20\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.6189 - tp: 1900.0000 - fp: 0.0000e+00 - tn: 7591.0000 - fn: 60.0000 - accuracy: 0.9937 - precision: 1.0000 - recall: 0.9694 - auc: 0.9997 - val_loss: 0.6176 - val_tp: 478.0000 - val_fp: 0.0000e+00 - val_tn: 1922.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 3/20\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.6157 - tp: 1948.0000 - fp: 0.0000e+00 - tn: 7603.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6156 - val_tp: 490.0000 - val_fp: 0.0000e+00 - val_tn: 1910.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 4/20\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6150 - tp: 1966.0000 - fp: 0.0000e+00 - tn: 7585.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6175 - val_tp: 478.0000 - val_fp: 0.0000e+00 - val_tn: 1922.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/20\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6164 - tp: 1929.0000 - fp: 0.0000e+00 - tn: 7622.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6202 - val_tp: 461.0000 - val_fp: 0.0000e+00 - val_tn: 1939.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/20\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6156 - tp: 1950.0000 - fp: 0.0000e+00 - tn: 7601.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6134 - val_tp: 504.0000 - val_fp: 0.0000e+00 - val_tn: 1896.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/20\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6156 - tp: 1951.0000 - fp: 0.0000e+00 - tn: 7600.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6183 - val_tp: 473.0000 - val_fp: 0.0000e+00 - val_tn: 1927.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/20\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6169 - tp: 1917.0000 - fp: 0.0000e+00 - tn: 7634.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6150 - val_tp: 494.0000 - val_fp: 0.0000e+00 - val_tn: 1906.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/20\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6151 - tp: 1962.0000 - fp: 0.0000e+00 - tn: 7589.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6177 - val_tp: 477.0000 - val_fp: 0.0000e+00 - val_tn: 1923.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/20\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6157 - tp: 1947.0000 - fp: 0.0000e+00 - tn: 7604.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6172 - val_tp: 480.0000 - val_fp: 0.0000e+00 - val_tn: 1920.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/20\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.6140 - tp: 1989.0000 - fp: 0.0000e+00 - tn: 7562.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6156 - val_tp: 490.0000 - val_fp: 0.0000e+00 - val_tn: 1910.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/20\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6158 - tp: 1945.0000 - fp: 0.0000e+00 - tn: 7606.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6145 - val_tp: 497.0000 - val_fp: 0.0000e+00 - val_tn: 1903.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/20\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.6164 - tp: 1930.0000 - fp: 0.0000e+00 - tn: 7621.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6167 - val_tp: 483.0000 - val_fp: 0.0000e+00 - val_tn: 1917.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/20\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6167 - tp: 1923.0000 - fp: 0.0000e+00 - tn: 7628.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6142 - val_tp: 499.0000 - val_fp: 0.0000e+00 - val_tn: 1901.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/20\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6171 - tp: 1912.0000 - fp: 0.0000e+00 - tn: 7639.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6118 - val_tp: 514.0000 - val_fp: 0.0000e+00 - val_tn: 1886.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 16/20\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6155 - tp: 1952.0000 - fp: 0.0000e+00 - tn: 7599.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6118 - val_tp: 514.0000 - val_fp: 0.0000e+00 - val_tn: 1886.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 17/20\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6170 - tp: 1915.0000 - fp: 0.0000e+00 - tn: 7636.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6126 - val_tp: 509.0000 - val_fp: 0.0000e+00 - val_tn: 1891.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 18/20\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.6161 - tp: 1937.0000 - fp: 0.0000e+00 - tn: 7614.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6191 - val_tp: 468.0000 - val_fp: 0.0000e+00 - val_tn: 1932.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 19/20\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6165 - tp: 1928.0000 - fp: 0.0000e+00 - tn: 7623.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6142 - val_tp: 499.0000 - val_fp: 0.0000e+00 - val_tn: 1901.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 20/20\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6148 - tp: 1969.0000 - fp: 0.0000e+00 - tn: 7582.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.6170 - val_tp: 481.0000 - val_fp: 0.0000e+00 - val_tn: 1919.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Average test loss: 0.6198811858892441)\n"
     ]
    }
   ],
   "source": [
    "model_history = ann_model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=validate_dataset, \n",
    "    callbacks=[cp_callback, tensorboard_callback],\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    workers=4)\n",
    "\n",
    "print(f'Average test loss: {np.average(model_history.history[\"loss\"])})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"binary_classifer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data_processing_layer (DataP multiple                  0         \n",
      "_________________________________________________________________\n",
      "hiden_layer_0 (Dense)        multiple                  271872    \n",
      "_________________________________________________________________\n",
      "hiden_layer_1 (Dense)        multiple                  16512     \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         multiple                  129       \n",
      "=================================================================\n",
      "Total params: 288,513\n",
      "Trainable params: 288,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Load testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern='../data/dac/sample_test.txt', \n",
    "    batch_size=200,\n",
    "    num_epochs=1,\n",
    "    column_defaults=COLUMN_DEFAULTS,\n",
    "    column_names=COLUMNS, \n",
    "    label_name='lbl', \n",
    "    field_delim='\\t',\n",
    "    shuffle=True\n",
    ").shuffle(10, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_packed_dataset = testing_dataset.map(FeaturesExtraction(num_col=NUM_COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.6156516671180725,\n",
       " 'tp': 2438.0,\n",
       " 'fp': 0.0,\n",
       " 'tn': 9513.0,\n",
       " 'fn': 0.0,\n",
       " 'accuracy': 1.0,\n",
       " 'precision': 1.0,\n",
       " 'recall': 1.0,\n",
       " 'auc': 1.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model.evaluate(testing_packed_dataset, return_dict=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binary_classifier_lab",
   "language": "python",
   "name": "binary_classifier_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
